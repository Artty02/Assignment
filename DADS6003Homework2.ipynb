{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmhn4XbMMgisyv8TUSM+GX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Artty02/Assignment/blob/main/DADS6003Homework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Gradient descent (Multiple linear regression)"
      ],
      "metadata": {
        "id": "k7vrOTZ0ilKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "#x, y = make_regression(n_samples=10, n_features=2, noise=2, random_state=123)\n",
        "#x = np.array([[0,2,3],[1,6,8]]).T\n",
        "x = np.array([[0,1],[2,6],[3,8]]) #x1, x2\n",
        "y = np.array([1,1,4])\n",
        "\n",
        "def cost_function(theta, x, y, N):\n",
        "  y_hat = x.dot(theta)\n",
        "  c = (1/(2*N))*np.sum((y_hat-y)**2)\n",
        "  return c\n",
        "\n",
        "def gradient_descent(alpha, x, y, ep=0.001, max_iter=10000): # Fixed: Indentation for function body\n",
        "  converged = False\n",
        "  iter = 0\n",
        "  N = x.shape[0] # number of samples\n",
        "  print(\"Num of data = \",N)\n",
        "\n",
        "  # initial theta\n",
        "  theta =  np.random.random((x.shape[1],1))\n",
        "  print(\"Init theta.shape = \",theta.shape)\n",
        "\n",
        "  # total error, J(theta)\n",
        "  J = cost_function(theta, x, y, N)\n",
        "  print(\"First J = \",J)\n",
        "\n",
        "  # Iterate Loop\n",
        "  while not converged:\n",
        "\n",
        "    y_hat = x.dot(theta)\n",
        "    diff = y_hat - y\n",
        "    grad = x.T.dot(diff)\n",
        "\n",
        "    theta = theta - alpha * (1/N) * (grad)\n",
        "\n",
        "    assert theta.shape == (3,1) #This line makes sure that the shape of theta is still be the same.\n",
        "\n",
        "    # error\n",
        "    J2 = cost_function(theta, x, y, N)\n",
        "\n",
        "    if abs(J-J2) <= ep:\n",
        "        print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
        "        converged = True\n",
        "\n",
        "    J = J2   # update error s\n",
        "    iter += 1  # update iter\n",
        "\n",
        "    if iter == max_iter:\n",
        "        print('       Max iterations exceeded!')\n",
        "        converged = True\n",
        "\n",
        "  #print(\"End converged iter = \",iter)\n",
        "  return theta\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  print(\"start main\")\n",
        "  print(x_b.shape)\n",
        "  y = y.reshape(-1,1)\n",
        "  print(y.shape)\n",
        "\n",
        "  alpha = 0.01 # learning rate\n",
        "  #Training process\n",
        "  theta = gradient_descent(alpha, x_b, y, ep=0.000000000001, max_iter=1000000)\n",
        "  print (\"Theta = \", theta)\n",
        "\n",
        "  #predict trainned x\n",
        "  xtest = np.array([[4,9]])\n",
        "  xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "  y_p = xtest_b.dot(theta)\n",
        "  print(\"y predict = \",y_p)"
      ],
      "metadata": {
        "id": "NaPAKj4VuFhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3dbefb1-d28c-44b3-9578-530f43729ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start main\n",
            "(3, 3)\n",
            "(3, 1)\n",
            "Num of data =  3\n",
            "Init theta.shape =  (3, 1)\n",
            "First J =  16.290204819557637\n",
            "       Converged, iterations:  276864 / 1000000\n",
            "Theta =  [[ 7.]\n",
            " [15.]\n",
            " [-6.]]\n",
            "y predict =  [[13.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient descent (Multiple linear regression)"
      ],
      "metadata": {
        "id": "SGC2k2Qgizr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "def cost_function(theta, x, y):\n",
        "    y_hat = x.dot(theta)\n",
        "    c = (1 / 2) * (y_hat - y)**2\n",
        "    return c\n",
        "\n",
        "def stochastic_gradient_descent(alpha, x, y, ep=0.001, max_iter=10000):\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    N = x.shape[0] # number of samples\n",
        "    print(\"Num of data = \", N)\n",
        "\n",
        "    # Initial theta (shape depends on number of features)\n",
        "    theta = np.random.random((x.shape[1], 1))\n",
        "    print(\"Init theta.shape = \", theta.shape)\n",
        "\n",
        "    # Initial cost\n",
        "    J = cost_function(theta, x[0], y[0]) # Calculate cost for the first sample\n",
        "    print(\"First J = \", J)\n",
        "\n",
        "    while not converged:\n",
        "        # Iterate through each data point\n",
        "        for i in range(N):\n",
        "            # Select a single data point\n",
        "            x_i = x[i].reshape(1, -1)  # Reshape for dot product\n",
        "            y_i = y[i]\n",
        "\n",
        "            y_hat = x_i.dot(theta)\n",
        "            diff = y_hat - y_i\n",
        "            grad = x_i.T * diff\n",
        "\n",
        "            theta = theta - alpha * grad\n",
        "\n",
        "            # Dynamic Assertion: Check if theta shape remains consistent\n",
        "            assert theta.shape == (x.shape[1], 1), f\"Unexpected theta shape: {theta.shape}\"\n",
        "\n",
        "        # Calculate total cost after each epoch (pass through all data)\n",
        "        J2 = 0\n",
        "        for j in range(N):\n",
        "            J2 += cost_function(theta, x[j], y[j])\n",
        "        J2 /= N\n",
        "\n",
        "        if abs(J - J2) <= ep:\n",
        "            print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
        "            converged = True\n",
        "            break\n",
        "\n",
        "        J = J2\n",
        "        iter += 1\n",
        "\n",
        "        if iter == max_iter:\n",
        "            print('       Max iterations exceeded!')\n",
        "            converged = True\n",
        "            break\n",
        "\n",
        "    return theta\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"start main\")\n",
        "\n",
        "    # Example data (add bias column to x)\n",
        "    x = np.array([[0, 1], [2, 6], [3, 8]])\n",
        "    x_b = np.c_[np.ones((x.shape[0], 1)), x]\n",
        "    y = np.array([1, 1, 4]).reshape(-1, 1)\n",
        "\n",
        "    print(x_b.shape)\n",
        "    print(y.shape)\n",
        "\n",
        "    alpha = 0.01  # Learning rate\n",
        "\n",
        "    theta = stochastic_gradient_descent(alpha, x_b, y, ep=1e-5, max_iter=100000)\n",
        "    print(\"Theta = \", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdNT2nbplda8",
        "outputId": "b3e47815-fbce-4de6-833d-0b23bb761d10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start main\n",
            "(3, 3)\n",
            "(3, 1)\n",
            "Num of data =  3\n",
            "Init theta.shape =  (3, 1)\n",
            "First J =  [0.05]\n",
            "       Converged, iterations:  10561 / 100000\n",
            "Theta =  [[ 5.12]\n",
            " [11.02]\n",
            " [-4.28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini-batch Gradient descent (Multiple linear regression)"
      ],
      "metadata": {
        "id": "y9mo81tRi386"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "def cost_function(theta, x, y):\n",
        "    y_hat = x.dot(theta)\n",
        "    c = (1 / (2 * len(y))) * np.sum((y_hat - y)**2)\n",
        "    return c\n",
        "\n",
        "def mini_batch_gradient_descent(alpha, x, y, batch_size, ep=0.001, max_iter=10000):\n",
        "    converged = False\n",
        "    iter = 0\n",
        "    N = x.shape[0]  # Number of samples\n",
        "    print(\"Num of data = \", N)\n",
        "\n",
        "    # Initial theta (shape depends on the number of features)\n",
        "    theta = np.random.random((x.shape[1], 1))\n",
        "    print(\"Init theta.shape = \", theta.shape)\n",
        "\n",
        "    # Initial cost\n",
        "    J = cost_function(theta, x, y)\n",
        "    print(\"First J = \", J)\n",
        "\n",
        "    while not converged:\n",
        "        # Shuffle the data at the start of each epoch\n",
        "        shuffled_indices = np.random.permutation(N)\n",
        "        x_shuffled = x[shuffled_indices]\n",
        "        y_shuffled = y[shuffled_indices]\n",
        "\n",
        "        for i in range(0, N, batch_size):\n",
        "            # Get the current mini-batch\n",
        "            x_batch = x_shuffled[i:i + batch_size]\n",
        "            y_batch = y_shuffled[i:i + batch_size]\n",
        "\n",
        "            y_hat = x_batch.dot(theta)\n",
        "            diff = y_hat - y_batch\n",
        "            grad = x_batch.T.dot(diff) / len(y_batch)\n",
        "\n",
        "            theta = theta - alpha * grad\n",
        "\n",
        "            # Dynamic Assertion: Check if theta shape remains consistent\n",
        "            assert theta.shape == (x.shape[1], 1), f\"Unexpected theta shape: {theta.shape}\"\n",
        "\n",
        "        # Calculate total cost after each epoch\n",
        "        J2 = cost_function(theta, x, y)\n",
        "\n",
        "        if abs(J - J2) <= ep:\n",
        "            print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
        "            converged = True\n",
        "            break\n",
        "\n",
        "        J = J2\n",
        "        iter += 1\n",
        "\n",
        "        if iter == max_iter:\n",
        "            print('       Max iterations exceeded!')\n",
        "            converged = True\n",
        "            break\n",
        "\n",
        "    return theta\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"start main\")\n",
        "\n",
        "    # Example data (add bias column to x)\n",
        "    x = np.array([[0, 1], [2, 6], [3, 8]])\n",
        "    x_b = np.c_[np.ones((x.shape[0], 1)), x]\n",
        "    y = np.array([1, 1, 4]).reshape(-1, 1)\n",
        "\n",
        "    print(x_b.shape)\n",
        "    print(y.shape)\n",
        "\n",
        "    alpha = 0.01  # Learning rate\n",
        "    batch_size = 2  # Desired mini-batch size\n",
        "\n",
        "    theta = mini_batch_gradient_descent(alpha, x_b, y, batch_size, ep=1e-5, max_iter=100000)\n",
        "    print(\"Theta = \", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXewfOCnmWDl",
        "outputId": "60958a5f-0a69-45e0-baaa-e95dfa84d6af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start main\n",
            "(3, 3)\n",
            "(3, 1)\n",
            "Num of data =  3\n",
            "Init theta.shape =  (3, 1)\n",
            "First J =  4.507840914856018\n",
            "       Converged, iterations:  968 / 100000\n",
            "Theta =  [[ 0.86]\n",
            " [ 1.68]\n",
            " [-0.34]]\n"
          ]
        }
      ]
    }
  ]
}